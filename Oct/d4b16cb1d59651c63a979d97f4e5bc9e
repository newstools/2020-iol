By Prof Louis C H Fourie is a Futurist and Technology Strategist. JOHANNESBURG - Over the last few years Artificial Intelligence (AI) has been incorporated into more and more devices that are part of our daily life. Currently, AI makes numerous important decisions on a regular basis and also performs countless automated functions. AI has indeed become an inherent and indispensable part of modern business. It is therefore understandable that many people are concerned about the future of AI and that it might one day destroy humanity. Although it is doubtful if humanity will be destroyed through AI and intelligent killer robots, unfortunately the shady effects of AI are already present and in many instances impacting our lives, often causing a divide amongst people and groups, inadvertently marginalising certain people, ensnaring our attention, and enlarging the gap between rich and the poor. The problem of algorithm bias About three decades ago, when algorithms were mostly used by computer scientists and did not really had an impact on our lives, algorithm bias was not a problem. But AI has since found its way into more sensitive areas such as loan application processing, the analysing of interviews and making of decisions regarding appointments of employees, adaptive pricing, credit scoring, facial recognition, health care and housing. There are many instances of algorithm bias that have been discovered over the past few years. A few recent examples are: All the above problems were eventually fixed, or the algorithms were discontinued, since algorithmic fairness is critical in the use of AI. One of the problems with algorithmic bias is the severe limitation that you cannot reason with an algorithm. Once the untransparent decision has been made by the algorithmic overlord, little can be done. The algorithm is in control. The importance of the training data All the above examples clearly illustrate the importance of the data that is used for machine learning. Biased data sources used for the training of the algorithm produce biased results in automated systems. Because AI systems learn to make decisions by looking at historical data, they often perpetuate existing biases. Machine and deep learning are especially susceptible to bias. The aim of deep learning is to find patterns in the data that it is trained on. Unfortunately, the data may reaffirm false stereotypes, for instance, where men are associated with doctors and women with nurses, the algorithm will apply this bias to answer all future questions. In the field of medicine, such as the diagnosis of skin cancer or determining the best drug treatment based on biological markers, these biases can mean the difference between life and death. Like-minded coders Unfortunately, technology is never neutral and mostly contains the fingerprints of its creators. This is also true in the case of the coding of algorithms, which often reflect the unconscious biases, preferences, and blind spots of the coders or algorithm creators. In many of the above examples of AI bias it was found that the algorithms were usually built by a relatively small, insulated group of like-minded young people from more or less similar social backgrounds and ethnic groups. As with any insulated team that work closely together, their unconscious biases and myopia often become new systems of belief and accepted behaviour as time passes, and even find its way into their products. The like-mindedness is created by universities that feed the major AI companies. Universities and computer science programmes mostly focus on hard software engineering skills, programming, systems engineering, mathematics, machine learning algorithms, natural language processing, computer vision, and other technical skills. There is almost no time for anthropology, philosophy, psychology, let alone, ethics. If they are offered, they are optional. And when these people start creating AI systems, they are sometimes blind to their own biases and would not know where to look for problems. Fixing the bias Companies and government agencies often introduce automated AI systems to cut costs and handle complex datasets, but unfortunately some of the algorithms are opaque and unregulated and contain biases that were often unintentionally built into their code. Fortunately, it is possible to fix the bias. Either with a totally new set of data and careful training of the algorithm, an improved neural network, or just by changing the very thing that the algorithm is supposed to predict. In the case of the above-mentioned healthcare system, researchers found that by focusing on only a subset of the health costs, such as emergency room visits, they were able to lower the bias. In fact, they also found that an algorithm that directly predicts health outcomes, rather than costs (as a proxy for patient health) is much more accurate. In the longer-term, universities will have to rethink their current computer science programmes and at least accommodate ethics as a core part of the curriculum, just as it was done in business studies some years ago after the Enron scandal. The only problem is that technology is moving faster than academia and infinitely faster than the Department of Higher Education and the Council of Higher Education that take a year or more to approve any new university curriculum. An ethics course without current material or the newest technological thinking just will not make sense. But whatever it takes, we urgently need an ethical conscience in the field of algorithms and AI decision-making to ensure that in future our lives are not ruled by a biased algorithmic overlord. Prof Louis C H Fourie is a Futurist and Technology Strategist. BUSINESS REPORT